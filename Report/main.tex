\documentclass[a4paper]{article}
\usepackage{a4wide,amssymb,epsfig,latexsym,multicol,array,hhline,fancyhdr}
%\usepackage{vntex}
\usepackage{amsmath}
\usepackage{lastpage}
\usepackage[lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{enumerate}
\usepackage{color}
\usepackage{graphicx}							% Standard graphics package
\usepackage{array}
\usepackage{tabularx, caption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{rotating}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{epsfig}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usetikzlibrary{arrows,snakes,backgrounds}
\usepackage{hyperref}
\hypersetup{urlcolor=blue,linkcolor=black,citecolor=black,colorlinks=true} 
%\usepackage{pstcol} 								% PSTricks with the standard color package
\usepackage{float}
\usepackage[none]{hyphenat}

\newtheorem{theorem}{{\bf Theorem}}
\newtheorem{property}{{\bf Property}}
\newtheorem{proposition}{{\bf Proposition}}
\newtheorem{corollary}[proposition]{{\bf Corollary}}
\newtheorem{lemma}[proposition]{{\bf Lemma}}

\AtBeginDocument{\renewcommand*\contentsname{Contents}}
\AtBeginDocument{\renewcommand*\refname{References}}
%\usepackage{fancyhdr}
\setlength{\headheight}{40pt}
\pagestyle{fancy}
\fancyhead{} % clear all header fields
\fancyhead[L]{
 \begin{tabular}{rl}
    \begin{picture}(25,15)(0,0)
    \put(0,-8){\includegraphics[width=8mm, height=8mm]{hcmut.png}}
    %\put(0,-8){\epsfig{width=10mm,figure=hcmut.eps}}
   \end{picture}&
	%\includegraphics[width=8mm, height=8mm]{hcmut.png} & %
	\begin{tabular}{l}
		\textbf{\bf \ttfamily University of Technology, Ho Chi Minh City}\\
		\textbf{\bf \ttfamily Faculty of Computer Science and Engineering}
	\end{tabular} 	
 \end{tabular}
}
\fancyhead[R]{
	\begin{tabular}{l}
		\tiny \bf \\
		\tiny \bf 
	\end{tabular}  }
\fancyfoot{} % clear all footer fields
\fancyfoot[L]{\scriptsize \ttfamily Machine Learning - Semester 1 (2023 - 2024)}
\fancyfoot[R]{\scriptsize \ttfamily Page {\thepage}/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}


%%%
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}
\makeatletter
\newcounter {subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection .\@alph\c@subsubsubsection}
\newcommand\subsubsubsection{\@startsection{subsubsubsection}{4}{\z@}%
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {1.5ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\newcommand*\l@subsubsubsection{\@dottedtocline{3}{10.0em}{4.1em}}
\newcommand*{\subsubsubsectionmark}[1]{}
\makeatother

% Define colors for code listing
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Code listing style
\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}
\begin{document}

\begin{titlepage}
\begin{center}
VIETNAM NATIONAL UNIVERSITY, HO CHI MINH CITY \\
UNIVERSITY OF TECHNOLOGY \\
FACULTY OF COMPUTER SCIENCE AND ENGINEERING
\end{center}

\vspace{1cm}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=3cm]{hcmut.png}
\end{center}
\end{figure}

\vspace{1cm}


\begin{center}
\begin{tabular}{c}
\multicolumn{1}{l}{\textbf{{\Large Machine Learning - CO3117}}}\\
~~\\
\hline
\\
\multicolumn{1}{l}{\textbf{{\Large Assignment}}}\\\\
\\
\textbf{{\Huge Iris Flower Classification}}\\
\multicolumn{1}{l}
\textbf{{\Huge using several Machine Learning models}}\\
\\\\
\hline
\end{tabular}
\end{center}

\vspace{3cm}

\begin{table}[h]
\begin{tabular}{rrl}
\hspace{3 cm} & \textbf{{\ Instructor}}: & Nguyen Duc Dung \\
\hspace{3 cm} & \textbf{{\ Class}}: & CC01  \\
\hspace{3 cm} & \textbf{{\ Student}}: 
& Tran Anh Kiet - 2152147 \\ 
\\

& &   \\
& &   \\
\end{tabular}
\end{table}

\begin{center}
{\footnotesize HO CHI MINH CITY, DECEMBER 2023}
\end{center}
\end{titlepage}


%\thispagestyle{empty}

\newpage
	\tableofcontents
\newpage

\section{Introduction}
Irises influenced the design of the French fleur-de-lis, are commonly used in the Japanese art of flower arrangement known as Ikebana, and underlie the floral scents of the “essence of violet” perfume. They’re also the subject of this well-known machine learning project, in which you must create an ML model capable of sorting irises based on five factors into one of three classes, Iris Setosa, Iris Versicolour, and Iris Virginica.

To get started, the data set below includes 50 instances of each of the three iris classes for a total of 150 instances. While one of the classes is linearly separable, the other two are not. Our goal is to create a model capable of classifying each iris instance into the appropriate class based on four attributes: sepal length, sepal width, petal length, and petal width. 
\begin{figure}[h]
	\centering
	\begin{minipage}{0.32\textwidth}
		\centering
		\includegraphics[height=4cm]{picture/flower/Iris Setosa} % Adjust height as needed
		\caption{Iris Setosa}
		\label{fig:setosa}
	\end{minipage}\hfill
	\begin{minipage}{0.32\textwidth}
		\centering
		\includegraphics[height=4cm]{picture/flower/Iris Versicolour} % Adjust height as needed
		\caption{Iris Versicolour}
		\label{fig:versicolour}
	\end{minipage}\hfill
	\begin{minipage}{0.32\textwidth}
		\centering
		\includegraphics[height=4cm]{picture/flower/Iris Virginica} % Adjust height as needed
		\caption{Iris Virginica}
		\label{fig:virginica}
	\end{minipage}
\end{figure}

\section{Literature Review}
Discuss previous work and findings related to the Iris classification problem.
For classifying, i use 3 polular model in machine learning
\include{Literature.tex}
\section{Methodology}
\subsection{Data Collection}
The dataset was obtained from the UCI Machine Learning Repository. For more information, visit the \href{https://archive.ics.uci.edu/ml/datasets/iris}{UCI Iris Dataset} page.\\
This is one of the earliest datasets used in the literature on classification methods and widely used in statistics and machine learning.  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are not linearly separable from each other.

Predicted attribute: class of iris plant.

This is an exceedingly simple domain.

\subsection{Data Preprocessing}
\begin{enumerate}
	\item \textbf{Loading the Dataset:} The dataset is loaded into a pandas DataFrame. This dataset includes features like sepal length, sepal width, petal length, and petal width, along with the class labels.
	\item \textbf{Feature Selection:} The features (sepal length, sepal width, petal length, and petal width) are separated from the class labels. This is done by splitting the DataFrame into X (features) and y (labels).
	\item \textbf{Splitting the Dataset:} The dataset is split into training and test sets.
	\textit{train\_test\_split()}. This is a common practice in machine learning to evaluate the model on data that it hasn't seen during training. The test size is set to 20\% of the total dataset.
	
	\item \textbf{Feature Scaling:} Standardization of features using \textit{StandardScaler()}.
\end{enumerate}
The Python code for these steps is as follows:

\begin{lstlisting}[language=Python]
	import pandas as pd
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import StandardScaler
	
	# Load dataset
	url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
	names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']
	dataset = pd.read_csv(url, names=names)
	
	# Feature Selection
	X = dataset.iloc[:, :-1].values
	y = dataset.iloc[:, -1].values
	
	# Splitting the Dataset
	X_train, X_test, y_train
	y_test = train_test_split(X, y, test_size=0.2, random_state=1)
	
	# Feature Scaling
	scaler = StandardScaler()
	X_train = scaler.fit_transform(X_train)
	X_test = scaler.transform(X_test)
\end{lstlisting}

\subsection{Model Training}

The training of different models in this project is outlined as follows:

\begin{enumerate}
	\item \textbf{K-nearest Neighbors (K-NN)}: 
	For K-NN, the number of neighbors is set to 5.We take 20\% of data for testing
	\begin{itemize}
		\item \textbf{For Classification:}
		The K-NN model is trained (fit) on the training data (X\_train, y\_train). The trained model is then used to make predictions on the test set (X\_test). The model's performance is evaluated using a confusion matrix and a classification report, which provide insights into accuracy and other metrics. The `model.predict(X\_test)` method is used to apply the algorithm to each point in the test set, making predictions based on the similarity of these points to the training set.
		
		\item \textbf{For Regression:}
		Two approaches can be used for weight assignment:
		\begin{enumerate}
			\item \textit{Uniform Weights} (`weights='uniform'`): Each of the 'k' nearest neighbors contributes equally to the prediction, regardless of their distance from the query point. For instance, in a KNN regression with k=5 and uniform weights, the predicted value is the average of the values of the 5 nearest neighbors.
			
			\item \textit{Distance Weights} (`weights='distance'`): Neighbors contribute to the prediction according to their distance from the query point. Closer neighbors have a greater influence on the prediction than those farther away, with the predicted value typically being a weighted average of the nearest neighbors' values, where weights are inversely proportional to the distance.
		\end{enumerate}
	\end{itemize}
	
	\item \textbf{Random Forest}: 
	The number of random tree is set to 100, we take 40\% of data for testing
	\begin{itemize}
\item \textbf{Classification}:
		Bootstrapping: Random Forest creates multiple decision trees using random subsets of training data, a process known as bootstrapping.
		
		Feature Randomness: Each tree is built using a random subset of features, increasing diversity and robustness among the trees.
		
		Building Decision Trees: Trees are formed by repeatedly splitting data based on features, using criteria like Gini impurity or entropy.
		
		Independence of Trees: Trees grow independently, reducing correlation and capturing a wide range of data patterns.
		
		Aggregation of Results: The forest combines tree predictions, usually through majority voting, to improve accuracy and generalization.
		
		Handling Overfitting: By averaging multiple trees, Random Forest reduces overfitting compared to individual decision trees.
\item \textbf{Random Forest Regressor}:
In this analysis, we focus on the Iris Setosa species from the Iris dataset, specifically examining the relationship between the sepal length and the sepal width. A Random Forest Regressor, consisting of 100 decision trees, is employed to model this relationship. The dataset is first filtered to include only the data points corresponding to Iris Setosa. Subsequently, 'sepal-length' is used as the feature to predict 'sepal-width'. The data is split into training and test sets, with the training set used to fit the Random Forest model. The model's performance is then visualized through a plot, which displays both the actual data points and the predictions made by the regressor. This approach provides insights into how sepal length influences sepal width in the Iris Setosa species and demonstrates the application of Random Forest Regression on a subset of a dataset.		
	\end{itemize}


    \item \textbf{Logistic Regression}: We allocate 50\% of the dataset for testing purposes.

\begin{itemize}
	\item \textbf{Classification}:
	The One-vs-Rest (OvR) strategy is employed, wherein separate logistic regression classifiers are constructed for each class to effectively distinguish it from the remaining classes. In relation to the Iris dataset, this results in the formulation of three distinct models:
	
	\begin{itemize}
		\item Model 1: Distinguishes Iris Setosa from the other species.
		\item Model 2: Identifies Iris Versicolor as opposed to other variants.
		\item Model 3: Differentiates Iris Virginica from the rest.
	\end{itemize}
	
	During the prediction phase, all three models are concurrently executed, and the model offering the highest probability for a class is used to categorize the instance.
	
	\item \textbf{Regression Analysis}:
	For a visual representation, a pairwise feature plot is created for two attributes of the Iris dataset, specifically 'sepal length' and 'sepal width'. This method visualizes the decision boundaries in the space defined by these two features and does not represent the full complexity of the data.
\end{itemize}

\end{enumerate}

\section{Results and Discussion}
First, let plot all the data:
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{picture/data}
	\caption{Data with feature}
	\label{fig:example}
\end{figure}
\subsection{Model Performance and Analysis}

\subsubsection{K-nearest neighbors}
\begin{enumerate}
	\item \textbf{Classifier Analysis}: The final KNN classifier achieved an impressive accuracy of 97\% on the test dataset, comprising 30 samples. This high accuracy indicates the model's effectiveness in classifying iris species based on their features.
\begin{figure}[h]
	\begin{minipage}{0.32\textwidth}
	\centering
	\includegraphics[height=4.2cm]{picture/KNN_confusion_matrix} % Adjust height as needed
	\caption{Confusion matrix}
	\label{fig:setosa}
\end{minipage}\hspace{3cm}
\begin{minipage}{0.32\textwidth}
	\centering
	\includegraphics[height=3cm]{picture/KNN_Classifier_result} % Adjust height as needed
	\caption{Output result}
	\label{fig:versicolour}
\end{minipage}\hfill
\end{figure}

The confusion matrix further details the model's performance, revealing that only one instance of the Versicolor species was incorrectly classified. This insight is crucial for understanding the model's strengths and limitations in distinguishing between species.

\item \textbf{Regression Analysis}: For a regression perspective, the 'petal-length' feature was used to predict the species, converting species names to numerical values.
	\begin{figure}[h]
		\centering
		\includegraphics[width=1\textwidth]{picture/KNN_Regression_petal}
		\caption{Final result}
		\label{fig:example}
	\end{figure}
\end{enumerate}
 The results showed distinct clusters for each species after standard scaling of the data. Petal lengths below -1 were classified as Iris Setosa, lengths between -0.5 and 1 as Versicolor, and lengths above 0.5 as Iris Virginica. This demonstrates the relationship between petal length and species, highlighting the model's ability to discern patterns within the data.
\subsubsection{Random Forest Analysis}
The Random Forest algorithm was applied in two distinct modes: classification and regression. This section outlines the results obtained from both approaches.
\begin{enumerate}
\item \textbf{Classification Analysis}
The classification model was evaluated using a confusion matrix, which is depicted in Figure \ref{fig:setosa}. The model achieved an overall prediction accuracy of 97\%. However, it is noteworthy that there were misclassifications, specifically one instance of Iris virginica and one of Iris versicolor being incorrectly classified.

\begin{figure}[h]
	\begin{minipage}{0.32\textwidth}
		\centering
		\includegraphics[height=4.2cm]{picture/RF2} % Adjust height as needed
		\caption{Confusion matrix for Random Forest Classifier}
		\label{fig:setosa}
	\end{minipage}\hspace{3cm}
	\begin{minipage}{0.32\textwidth}
		\centering
		\includegraphics[height=3cm]{picture/RF3} % Adjust height as needed
		\caption{Classification output results}
		\label{fig:versicolour}
	\end{minipage}\hfill
\end{figure}
\item \textbf{Regression Analysis}
In the regression analysis, the focus was on predicting the sepal width based on the sepal length specifically for the Iris Setosa species. The Random Forest Regressor, though effective, exhibited limitations in extrapolating values beyond the range of the training data. The model generally predicted values close to the maximum of the training set range when confronted with inputs outside this range. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{picture/RF5}
	\caption{Random Forest Regression Analysis of Sepal Width Based on Sepal Length for Iris Setosa}
	\label{fig:iris_setosa_regression}
\end{figure}
\end{enumerate}
\newpage
\subsubsection{Logistic Regression}
\begin{enumerate}
	\item \textbf{Classification Analysis}
\begin{figure}[h]
	\begin{minipage}{0.32\textwidth}
		\centering
		\includegraphics[height=4.2cm]{picture/LR2} % Adjust height as needed
		\caption{Confusion matrix for Random Forest Classifier}
		\label{fig:setosa}
	\end{minipage}\hspace{3cm}
	\begin{minipage}{0.32\textwidth}
		\centering
		\includegraphics[height=3cm]{picture/LR3} % Adjust height as needed
		\caption{Classification output results}
		\label{fig:versicolour}
	\end{minipage}\hfill
\end{figure}
	\item \textbf{Regression Analysis}
	\begin{figure}[h]
		\centering
		\includegraphics[width=1\textwidth]{picture/LR4}
		\caption{Logistic Regression Sigmoid Function}
		\label{fig:logistic_regression}
	\end{figure}
\end{enumerate}
\subsubsection{Other Reference Model}
In here, i present some other model for visualization purpose:
\begin{figure}[h]
	\begin{minipage}{0.32\textwidth}
		\centering
		\includegraphics[height=4.2cm]{picture/Anomaly} % Adjust height as needed
		\caption{Confusion matrix for Random Forest Classifier}
		\label{fig:setosa}
	\end{minipage}\hspace{3cm}
	\begin{minipage}{0.32\textwidth}
		\centering
		\includegraphics[height=4.2cm]{picture/k_mean} % Adjust height as needed
		\caption{Classification output results}
		\label{fig:versicolour}
	\end{minipage}\hfill
\end{figure}
\newpage
\section{Conclusion}
Github link for this project:
\href{https://github.com/illubaby/ML-Iris-Classification}{ML-Iris-Classification}\\
In this study, i successfully developed and evaluated a model for the classification of iris flowers into three species - Setosa, Versicolor, and Virginica - based on their physical attributes: sepal length, sepal width, petal length, and petal width. Our work encapsulates the classic machine learning task of classification, leveraging the widely-recognized Iris dataset. This dataset, comprising 150 instances with four feature measurements and a corresponding species label, served as the foundation for our analytical approach.

Throughout the project, our primary objective was to harness the predictive power of machine learning to accurately categorize each iris flower into its appropriate species. To achieve this, we employed Python and the pandas library for data manipulation and preliminary analysis, ensuring a robust understanding of the dataset's characteristics and potential challenges.

Upon thorough exploration, we observed distinct patterns and relationships within the data, which guided our choice of machine learning models. The simplicity and effectiveness of our chosen model are evident in its ability to dissect the nuanced differences between the species, based on the given attributes.

This project not only demonstrates the practical application of machine learning algorithms in biological classification but also serves as a vital learning experience in data analysis, model selection, and performance evaluation. The insights gained here can be extended to more complex classification tasks, with potential adaptations and enhancements to tackle datasets exhibiting higher dimensions and variability.

In conclusion, the Iris Flower Classification project stands as a testament to the power and versatility of machine learning in transforming raw data into meaningful knowledge, driving forward the fields of data science and computational biology.

\section{References}
\href{https://machinelearningcoban.com/2017/01/08/knn/}{https://machinelearningcoban.com/2017/01/08/knn/}\\
\href{https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning}{https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning}\\
\href{https://www.analyticsvidhya.com/blog/2021/10/everything-you-need-to-know-about-linear-regression/}{https://www.analyticsvidhya.com/blog/2021/10/everything-you-need-to-know-about-linear-regression/}\\
\href{https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-logistic-regression/}{https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-logistic-regression/}
\end{document}

